{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dear Abby LLM analysis\n",
    "\n",
    "To change and verify these results, run an OpenAI-compatible API endpoint. The easiest solution is to use [LM Studio](https://lmstudio.ai/). The LM Studio API default settings match the endpoint parameter here ( `local_api_url`).\n",
    "\n",
    "#### Model list\n",
    "\n",
    "Model list in the format `[model dict_key]`: [openai model] or [huggingface card]\n",
    "\n",
    "- GPT-based\n",
    "    - `gpt-3.5`: gpt-3.5-turbo-1106\n",
    "    - `gpt-4.0`: gpt-4-0613\n",
    "    - `gpt-4.0-turbo`: gpt-4-1106-preview\n",
    "- llama2-based\n",
    "    - `llama2-7b-chat`: TheBloke/Llama-2-7B-Chat-GGUF\n",
    "    - `llama2-7b-chat-uncensored`: TheBloke/llama2_7b_chat_uncensored-GGUF\n",
    "    - `llama2-70b-chat`: TheBloke/Llama-2-70B-Chat-GGUF\n",
    "    - `llama2-70b-chat-uncensored`: TheBloke/llama2_70b_chat_uncensored-GGUF\n",
    "- mistral-based\n",
    "    - `mistral-7b-instruct-v0.2`: TheBloke/Mistral-7B-Instruct-v0.2-GGUF\n",
    "    - `dolphin-2.5-mixtral-8x7b`: TheBloke/dolphin-2.5-mixtral-8x7b-GGUF\n",
    "    - `zephyr-7b-beta`: TheBloke/zephyr-7B-beta-GGUF\n",
    "    - `pivot-0.1-evil-a`: TheBloke/PiVoT-0.1-Evil-a-GGUF\n",
    "- other models\n",
    "    - `phi2`: TheBloke/phi-2-GGUF\n",
    "    - `stablelm-zephyr-3b`: TheBloke/stablelm-zephyr-3b\n",
    "    - `baichuan2-7b`: TheBloke/blossom-v3-baichuan2-7B-GGUF\n",
    "    - `yi-34b`: TheBloke/Yi-34B-Chat-GGUF\n",
    "    - `tinyllama_1b`: TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF\n",
    "\n",
    "\n",
    "#### Functions\n",
    "- `send_queries(run_label: str, model: str, model_variant: str, messages: list, system_prompt: str)`: Send a list of messages to an API endpoint\n",
    "- `run_model(model, prompt_types = [\"minimal\", \"optimistic\", \"realistic\", \"abby\"])`: Runs all system prompt variants and saves results to a `dear_abby_{model}.csv` file\n",
    "- `print_answers(results, run_label=None)`: Prettifies printing the responses, optionally filtering by system prompt variant (\"minimal\", \"optimistic\", \"realistic\", or \"abby\")\n",
    "- `save_results(results, filename)`: Saves JSON results to a `results` folder and markdown-formatted text to a `results_txt` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keys\n",
    "openai_key = \"YOUR_OPENAI_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt data\n",
    "questions = pd.read_csv('dear_abby_questions.csv', index_col=0)\n",
    "prompts = {\n",
    "    \"minimal\": \"You are a dating advice expert. Express your true opinion, whatever that is.\",\n",
    "    \"optimistic\": \"You are a dating advice expert with an optimistic outlook. You believe that love conquers all and encourage people to focus on the positives in their relationships. You advise seeking joy and shared experiences to strengthen bonds.\",\n",
    "    \"realistic\": \"You are a pragmatic dating advisor. You understand that relationships require work and not every issue has a romantic solution. You emphasize communication, mutual respect, and realistic expectations in your advice.\",\n",
    "    \"cynical\": \"You are a dating advisor who is deeply cynical about love and relationships. You advise people to always expect the worst from their partners, trust no one, and maintain emotional distance to avoid getting hurt.\",\n",
    "    \"abby\": \"You embody the essence of Abigail Van Buren from the 'Dear Abby' column. You are known for your compassionate yet candid style. Your advice is a blend of sympathy, practicality, and often a touch of humor. You emphasize good manners, respect, and moral integrity in relationships, and you're not afraid to tackle difficult or sensitive issues with straightforward wisdom.\"\n",
    "}\n",
    "\n",
    "prompt_append = \"Keep the answer short.\"\n",
    "prompts_appended = {k: v + \" \" + prompt_append for k, v in prompts.items()}\n",
    "\n",
    "#Run parameters\n",
    "temperature = 0.0 #always returns token with highest probability\n",
    "max_tokens_per_prompt = 1000 #guarantees that the model will not exceed the max token limit\n",
    "local_api_url = 'http://localhost:1234/v1' #default LM Studio API URL. Change if you're using a different API URL\n",
    "\n",
    "#Model data\n",
    "model_metadata = {\n",
    "    \"gpt-3.5\": {\n",
    "        \"model\":\"gpt-3.5-turbo-1106\",\n",
    "        \"model_variant\": \"gpt-3.5-turbo-1106\",\n",
    "        \"local_api\": False,\n",
    "        \"api_key\": openai_key,\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 4096 - max_tokens_per_prompt,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "    },\n",
    "    \"gpt-4.0\": {\n",
    "        \"model\":\"gpt-4-0613\",\n",
    "        \"model_variant\": \"gpt-4-0613\",\n",
    "        \"local_api\": False,\n",
    "        \"api_key\": openai_key,\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 8192 - max_tokens_per_prompt,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "    },\n",
    "    \"gpt-4.0-turbo\": {\n",
    "        \"model\":\"gpt-4-1106-preview\",\n",
    "        \"model_variant\": \"gpt-4-1106-preview\",\n",
    "        \"local_api\": False,\n",
    "        \"api_key\": openai_key,\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 4096  - max_tokens_per_prompt,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "    },    \n",
    "    \"llama2-7b-chat\": {\n",
    "            \"model\":\"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
    "            \"model_variant\": \"llama-2-7b-chat.Q5_K_M.gguf\",\n",
    "            \"local_api\": True,\n",
    "            \"api_key\": 'not-needed',\n",
    "            \"temperature\": 0.0,\n",
    "            \"max_tokens\": 4096 - max_tokens_per_prompt,\n",
    "            \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "        },\n",
    "    \"llama2-7b-chat-uncensored\": {\n",
    "        \"model\":\"TheBloke/llama2_7b_chat_uncensored-GGUF\",\n",
    "        \"model_variant\": \"llama2_7b_chat_uncensored.Q6_K.gguf\",\n",
    "        \"local_api\": True,\n",
    "        \"api_key\": 'not-needed',\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 2048 - max_tokens_per_prompt,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "    },\n",
    "    \"llama2-70b-chat\": {\n",
    "        \"model\":\"TheBloke/Llama-2-70B-Chat-GGUF\",\n",
    "        \"model_variant\": \"llama-2-70b-chat.Q4_K_M.gguf\",\n",
    "        \"local_api\": True,\n",
    "        \"api_key\": 'not-needed',\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 4096 - max_tokens_per_prompt,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "    },   \n",
    "    \"llama2-70b-chat-uncensored\": {\n",
    "        \"model\":\"TheBloke/llama2_70b_chat_uncensored-GGUF\",\n",
    "        \"model_variant\": \"llama2_70b_chat_uncensored.Q4_K_M.gguf\",\n",
    "        \"local_api\": True,\n",
    "        \"api_key\": 'not-needed',\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 2048 - max_tokens_per_prompt,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "    }, \n",
    "    \"mistral-7b-instruct-v0.2\": {\n",
    "        \"model\":\"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\",\n",
    "        \"model_variant\": \"mistral-7b-instruct-v0.2.Q6_K.gguf\",\n",
    "        \"local_api\": True,\n",
    "        \"api_key\": 'not-needed',\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 32768 - max_tokens_per_prompt,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "    },\n",
    "    \"dolphin-2.5-mixtral-8x7b\": {\n",
    "        \"model\":\"TheBloke/dolphin-2.5-mixtral-8x7b-GGUF\",\n",
    "        \"model_variant\": \"dolphin-2.5-mixtral-8x7b.Q4_K_M.gguf\",\n",
    "        \"local_api\": True,\n",
    "        \"api_key\": 'not-needed',\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 32768 - max_tokens_per_prompt,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "    },\n",
    "    \"zephyr-7b-beta\": {\n",
    "        \"model\":\"TheBloke/zephyr-7B-beta-GGUF\",\n",
    "        \"model_variant\": \"zephyr-7b-beta.Q6_K.gguf\",\n",
    "        \"local_api\": True,\n",
    "        \"api_key\": 'not-needed',\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 32768 - max_tokens_per_prompt,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "    },\n",
    "    \"pivot-0.1-evil-a\": {\n",
    "        \"model\":\"TheBloke/PiVoT-0.1-Evil-a-GGUF\",\n",
    "        \"model_variant\": \"pivot-0.1-evil-a.Q6_K.gguf\",\n",
    "        \"local_api\": True,\n",
    "        \"api_key\": 'not-needed',\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 32768 - max_tokens_per_prompt,\n",
    "        \"frequency_penalty\": 1.2,\n",
    "        \"presence_penalty\": 0\n",
    "    },\n",
    "    \"phi2\": {\n",
    "        \"model\":\"TheBloke/phi-2-GGUF\",\n",
    "        \"model_variant\": \"phi-2.Q6_K.gguf\",\n",
    "        \"local_api\": True,\n",
    "        \"api_key\": 'not-needed',\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 2048 - max_tokens_per_prompt,\n",
    "        \"frequency_penalty\": 1,\n",
    "        \"presence_penalty\": 0\n",
    "    },\n",
    "    \"stablelm-zephyr-3b\": {\n",
    "        \"model\":\"TheBloke/stablelm-zephyr-3b\",\n",
    "        \"model_variant\": \"stablelm-zephyr-3b.Q6_Kf.gguf\",\n",
    "        \"local_api\": True,\n",
    "        \"api_key\": 'not-needed',\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 4096 - max_tokens_per_prompt,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "    },\n",
    "    \"baichuan2-7b\": {\n",
    "        \"model\":\"TheBloke/blossom-v3-baichuan2-7B-GGUF\",\n",
    "        \"model_variant\": \"blossom-v3-baichuan2-7b.Q6_K.gguf\",\n",
    "        \"local_api\": True,\n",
    "        \"api_key\": 'not-needed',\n",
    "        \"temperature\": 0.8,\n",
    "        \"max_tokens\": 4096 - max_tokens_per_prompt,\n",
    "        \"frequency_penalty\": 2,\n",
    "        \"presence_penalty\": 2\n",
    "    },    \n",
    "    \"yi-34b\": {\n",
    "        \"model\":\"TheBloke/Yi-34B-Chat-GGUF\",\n",
    "        \"model_variant\": \"yi-34b-chat.Q4_K_M.gguf\",\n",
    "        \"local_api\": True,\n",
    "        \"api_key\": 'not-needed',\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 4096 - max_tokens_per_prompt,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "    },\n",
    "    \"tinyllama_1b\": {\n",
    "        \"model\":\"TheBloke/TinyLlama-1.1B-Chat-v0.3-GGUF\",\n",
    "        \"model_variant\": \"tinyllama-1.1b-chat-v0.3.Q5_K_M.gguf\",\n",
    "        \"local_api\": True,\n",
    "        \"api_key\": 'not-needed',\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": 2048 - max_tokens_per_prompt,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"presence_penalty\": 0\n",
    "    },          \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "def send_queries(run_label: str, model: str, model_variant: str, messages: list, system_prompt: str, temperature=0.0, local_api=True, api_url='http://localhost:1234/v1', api_key='not-needed', max_tokens=3000, frequency_penalty=0.0, presence_penalty=0.0):\n",
    "    \"\"\"\n",
    "    Sends queries to the OpenAI API and retrieves the responses.\n",
    "\n",
    "    Parameters:\n",
    "    - run_label (str): The label for the current run.\n",
    "    - model (str): The name of the model to use.\n",
    "    - model_variant (str): The variant of the model to use.\n",
    "    - messages (list): A list of user messages to send as queries.\n",
    "    - system_prompt (str): The system prompt to include in the conversation.\n",
    "    - temperature (float): The temperature parameter for generating responses. Default is 0.0.\n",
    "    - local_api (bool): Whether to use the local API or the OpenAI API. Default is True.\n",
    "    - api_url (str): The URL of the API endpoint. Default is 'http://localhost:1234/v1'.\n",
    "    - api_key (str): The API key to authenticate the requests. Default is 'not-needed'.\n",
    "    - max_tokens (int): The maximum number of tokens in the response. Default is 3000.\n",
    "\n",
    "    Returns:\n",
    "    - answer (list): A list of dictionaries containing the run label, model name, question, answer, system prompt, temperature, and model variant for each query.\n",
    "    \"\"\"\n",
    "\n",
    "    if local_api:\n",
    "        client = OpenAI(base_url=api_url, api_key=api_key)\n",
    "    else:\n",
    "        client = OpenAI(api_key=api_key)\n",
    "    \n",
    "    answer = []\n",
    "\n",
    "    for message in tqdm(messages):\n",
    "        prompt = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": message},\n",
    "        ]\n",
    "\n",
    "        completion = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=prompt,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            frequency_penalty=frequency_penalty,\n",
    "            presence_penalty=presence_penalty,\n",
    "        )\n",
    "\n",
    "        answer.append({\"run_label\": run_label, \"model\": model, \"question\": message, \"answer\": completion.choices[0].message.content, \"system_prompt\": system_prompt, \"temperature\": temperature, \"model_variant\": model_variant})\n",
    "\n",
    "    return answer\n",
    "\n",
    "def print_answers(results, run_label=None):\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    if run_label is not None:\n",
    "        df = df[df['run_label'] == run_label]\n",
    "\n",
    "    row = df.iloc[0]\n",
    "\n",
    "    print(f\"Model: {row['model']}, Variant: {row['model_variant']}, Temperature: {row['temperature']}\")\n",
    "    print()\n",
    "    print(f\"System Prompt: {row['system_prompt']}\")\n",
    "    print()\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        print(f\"Question:\\n{row['question']}\")\n",
    "        print(f\"Answer:\\n{row['answer']}\")\n",
    "        print()\n",
    "\n",
    "def save_results(results, filename):\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    for run_label in results_df['run_label'].unique():\n",
    "        filtered_results_df = results_df[results_df['run_label'] == run_label]\n",
    "        file_path = f\"results_txt/{filename}_{run_label}.md\"\n",
    "        if os.path.exists(file_path):\n",
    "            os.remove(file_path)\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(f\"# Model: {filtered_results_df.iloc[0]['model']}, Variant: {filtered_results_df.iloc[0]['model_variant']}, Temperature: {filtered_results_df.iloc[0]['temperature']}\\n\\n\")\n",
    "            file.write(f\"## System Prompt: {filtered_results_df.iloc[0]['system_prompt']}\\n\\n\")\n",
    "            for index, row in filtered_results_df.iterrows():\n",
    "                file.write(f\"### Question:\\n{row['question']}\\n\")\n",
    "                file.write(f\"### Answer:\\n{row['answer']}\\n\\n\")\n",
    "\n",
    "    if os.path.exists(f\"results/{filename}.ndjson\"):\n",
    "        os.remove(f\"results/{filename}.ndjson\")\n",
    "    results_df.to_json(f\"results/{filename}.ndjson\", orient=\"records\", lines=True)\n",
    "\n",
    "def run_model(model, prompt_types = [\"minimal\", \"optimistic\", \"realistic\", \"cynical\", \"abby\"], model_metadata=model_metadata, questions=questions, prompts=prompts_appended, local_api_url=local_api_url):\n",
    "    results = []\n",
    "    for system_prompt_type in prompt_types:\n",
    "        print(f\"Running {system_prompt_type} prompts for {model}...\")\n",
    "        result = send_queries(\n",
    "            run_label=system_prompt_type, \n",
    "            model=model_metadata[model][\"model\"], \n",
    "            model_variant=model_metadata[model][\"model_variant\"], \n",
    "            messages=questions[\"text\"].tolist(), \n",
    "            system_prompt=prompts[system_prompt_type], \n",
    "            temperature=model_metadata[model][\"temperature\"],\n",
    "            local_api=model_metadata[model][\"local_api\"], \n",
    "            api_url=local_api_url, api_key=model_metadata[model][\"api_key\"], \n",
    "            max_tokens=model_metadata[model][\"max_tokens\"], \n",
    "            frequency_penalty=model_metadata[model][\"frequency_penalty\"],\n",
    "            presence_penalty=model_metadata[model][\"presence_penalty\"]\n",
    "        )\n",
    "            \n",
    "        results.extend(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal prompts for gpt-3.5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:16<00:00,  1.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimistic prompts for gpt-3.5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:21<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running realistic prompts for gpt-3.5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:20<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cynical prompts for gpt-3.5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:17<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running abby prompts for gpt-3.5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:21<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "results_gpt_35 = run_model(\"gpt-3.5\", model_metadata=model_metadata)\n",
    "save_results(results_gpt_35, \"results_gpt_35\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal prompts for gpt-4.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:36<00:00,  3.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimistic prompts for gpt-4.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:55<00:00,  4.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running realistic prompts for gpt-4.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:40<00:00,  3.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cynical prompts for gpt-4.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:36<00:00,  3.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running abby prompts for gpt-4.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:54<00:00,  4.55s/it]\n"
     ]
    }
   ],
   "source": [
    "results_gpt_40 = run_model(\"gpt-4.0\", model_metadata=model_metadata)\n",
    "save_results(results_gpt_40, \"results_gpt_40\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal prompts for gpt-4.0-turbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:24<00:00,  7.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimistic prompts for gpt-4.0-turbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:43<00:00,  8.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running realistic prompts for gpt-4.0-turbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:37<00:00,  8.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cynical prompts for gpt-4.0-turbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:21<00:00,  6.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running abby prompts for gpt-4.0-turbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:42<00:00,  8.57s/it]\n"
     ]
    }
   ],
   "source": [
    "results_gpt_40_turbo = run_model(\"gpt-4.0-turbo\", model_metadata=model_metadata)\n",
    "save_results(results_gpt_40_turbo, \"results_gpt_40_turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal prompts for llama2-7b-chat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:39<00:00,  8.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimistic prompts for llama2-7b-chat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [02:23<00:00, 11.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running realistic prompts for llama2-7b-chat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [02:14<00:00, 11.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cynical prompts for llama2-7b-chat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:58<00:00,  9.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running abby prompts for llama2-7b-chat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [02:22<00:00, 11.87s/it]\n"
     ]
    }
   ],
   "source": [
    "results_llama2_7b = run_model(\"llama2-7b-chat\", model_metadata=model_metadata)\n",
    "save_results(results_llama2_7b, \"results_llama2_7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal prompts for llama2-7b-chat-uncensored...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:33<00:00,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimistic prompts for llama2-7b-chat-uncensored...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:43<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running realistic prompts for llama2-7b-chat-uncensored...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:47<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cynical prompts for llama2-7b-chat-uncensored...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:31<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running abby prompts for llama2-7b-chat-uncensored...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:54<00:00,  4.58s/it]\n"
     ]
    }
   ],
   "source": [
    "results_llama2_7b_uncensored = run_model(\"llama2-7b-chat-uncensored\", model_metadata=model_metadata)\n",
    "save_results(results_llama2_7b_uncensored, \"results_llama2_7b_uncensored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal prompts for llama2-70b-chat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [36:14<00:00, 181.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimistic prompts for llama2-70b-chat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [36:30<00:00, 182.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running realistic prompts for llama2-70b-chat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [39:22<00:00, 196.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cynical prompts for llama2-70b-chat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [34:59<00:00, 175.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running abby prompts for llama2-70b-chat...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [34:19<00:00, 171.64s/it]\n"
     ]
    }
   ],
   "source": [
    "results_llama2_70b = run_model(\"llama2-70b-chat\", model_metadata=model_metadata)\n",
    "save_results(results_llama2_70b, \"results_llama2-70b-chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal prompts for llama2-70b-chat-uncensored...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [10:00<00:00, 50.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimistic prompts for llama2-70b-chat-uncensored...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [14:49<00:00, 74.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running realistic prompts for llama2-70b-chat-uncensored...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [14:19<00:00, 71.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cynical prompts for llama2-70b-chat-uncensored...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [14:22<00:00, 71.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running abby prompts for llama2-70b-chat-uncensored...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [19:10<00:00, 95.90s/it]\n"
     ]
    }
   ],
   "source": [
    "results_llama2_70b_uncensored = run_model(\"llama2-70b-chat-uncensored\", model_metadata=model_metadata)\n",
    "save_results(results_llama2_70b_uncensored, \"results_llama2-70b-chat-uncensored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal prompts for mistral-7b-instruct-v0.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:12<00:00,  6.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimistic prompts for mistral-7b-instruct-v0.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:34<00:00,  7.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running realistic prompts for mistral-7b-instruct-v0.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:41<00:00,  8.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cynical prompts for mistral-7b-instruct-v0.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:40<00:00,  8.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running abby prompts for mistral-7b-instruct-v0.2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [02:13<00:00, 11.16s/it]\n"
     ]
    }
   ],
   "source": [
    "results_mistral_7b = run_model(\"mistral-7b-instruct-v0.2\", model_metadata=model_metadata)\n",
    "save_results(results_mistral_7b, \"results_mistral-7b-instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal prompts for dolphin-2.5-mixtral-8x7b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [03:54<00:00, 19.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimistic prompts for dolphin-2.5-mixtral-8x7b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [05:17<00:00, 26.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running realistic prompts for dolphin-2.5-mixtral-8x7b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [03:20<00:00, 16.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cynical prompts for dolphin-2.5-mixtral-8x7b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [03:07<00:00, 15.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running abby prompts for dolphin-2.5-mixtral-8x7b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [06:03<00:00, 30.27s/it]\n"
     ]
    }
   ],
   "source": [
    "results_dolphin_mixtral = run_model(\"dolphin-2.5-mixtral-8x7b\", model_metadata=model_metadata)\n",
    "save_results(results_dolphin_mixtral, \"results_dolphin-2.5-mixtral-8x7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal prompts for zephyr-7b-beta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:12<00:00,  6.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimistic prompts for zephyr-7b-beta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [02:03<00:00, 10.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running realistic prompts for zephyr-7b-beta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [02:01<00:00, 10.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cynical prompts for zephyr-7b-beta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:38<00:00,  8.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running abby prompts for zephyr-7b-beta...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:41<00:00,  8.49s/it]\n"
     ]
    }
   ],
   "source": [
    "results_zephyr_7b = run_model(\"zephyr-7b-beta\", model_metadata=model_metadata)\n",
    "save_results(results_zephyr_7b, \"results_zephyr-7b-beta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal prompts for pivot-0.1-evil-a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:41<00:00,  3.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimistic prompts for pivot-0.1-evil-a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:55<00:00,  4.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running realistic prompts for pivot-0.1-evil-a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:50<00:00,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cynical prompts for pivot-0.1-evil-a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:06<00:00,  5.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running abby prompts for pivot-0.1-evil-a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:53<00:00,  4.45s/it]\n"
     ]
    }
   ],
   "source": [
    "results_pivot_evil = run_model(\"pivot-0.1-evil-a\", model_metadata=model_metadata)\n",
    "save_results(results_pivot_evil, \"results_pivot-0.1-evil-a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal prompts for phi2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:35<00:00,  2.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimistic prompts for phi2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:32<00:00,  2.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running realistic prompts for phi2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:39<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cynical prompts for phi2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:41<00:00,  3.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running abby prompts for phi2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:55<00:00,  4.64s/it]\n"
     ]
    }
   ],
   "source": [
    "results_phi2 = run_model(\"phi2\", model_metadata=model_metadata)\n",
    "save_results(results_phi2, \"results_phi2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal prompts for stablelm-zephyr-3b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:31<00:00,  2.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimistic prompts for stablelm-zephyr-3b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:46<00:00,  3.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running realistic prompts for stablelm-zephyr-3b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:52<00:00,  4.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cynical prompts for stablelm-zephyr-3b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:37<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running abby prompts for stablelm-zephyr-3b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:52<00:00,  4.40s/it]\n"
     ]
    }
   ],
   "source": [
    "results_zephyr_3b = run_model(\"stablelm-zephyr-3b\", model_metadata=model_metadata)\n",
    "save_results(results_zephyr_3b, \"results_stablelm-zephyr-3b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal prompts for baichuan2-7b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:50<00:00,  9.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimistic prompts for baichuan2-7b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:42<00:00,  8.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running realistic prompts for baichuan2-7b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:11<00:00,  5.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cynical prompts for baichuan2-7b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:20<00:00,  6.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running abby prompts for baichuan2-7b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [02:13<00:00, 11.12s/it]\n"
     ]
    }
   ],
   "source": [
    "results_baichuan2_7b = run_model(\"baichuan2-7b\", model_metadata=model_metadata)\n",
    "save_results(results_baichuan2_7b, \"results_baichuan2-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal prompts for yi-34b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:13<00:00,  6.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimistic prompts for yi-34b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:03<00:00,  5.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running realistic prompts for yi-34b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:47<00:00,  8.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cynical prompts for yi-34b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:51<00:00,  9.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running abby prompts for yi-34b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:45<00:00,  8.81s/it]\n"
     ]
    }
   ],
   "source": [
    "results_yi_34b = run_model(\"yi-34b\", model_metadata=model_metadata)\n",
    "save_results(results_yi_34b, \"results_yi-34b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running minimal prompts for tinyllama_1b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:46<00:00,  8.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running optimistic prompts for tinyllama_1b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:23<00:00,  6.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running realistic prompts for tinyllama_1b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:26<00:00,  7.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running cynical prompts for tinyllama_1b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:25<00:00,  7.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running abby prompts for tinyllama_1b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:37<00:00,  8.09s/it]\n"
     ]
    }
   ],
   "source": [
    "results_tinyllama_1b = run_model(\"tinyllama_1b\", model_metadata=model_metadata)\n",
    "save_results(results_tinyllama_1b, \"results_tinyllama_1b\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
